import pandas as pd
import nltk

# Ensure NLTK tokenizer is available
nltk.download('punkt')

import nltk
import pandas as pd

# Ensure NLTK tokenizer is available
nltk.download('punkt')

def convert_to_inline_format(text, spans):
    # Tokenize text
    tokens = nltk.word_tokenize(text)

    # Ensure punctuation is spaced correctly
    formatted_text = " ".join(tokens)

    # Handle cases where spans are missing or "NA"
    if pd.isna(spans) or spans == "NA":
        return formatted_text + "\n\n\n"

    # Process named entity spans
    entity_mappings = []
    spans_list = spans.split(";") if ";" in spans else [spans]  # Handle a single span case

    for span in spans_list:
        span = span.strip()
        if not span:
            continue

        try:
            entity_type, entity_text = span.split(": ", 1)
        except ValueError:
            print(f"Skipping malformed span: {span}")  # Debugging message
            continue

        # Tokenize entity text
        entity_tokens = nltk.word_tokenize(entity_text)
        entity_indices = []

        # Find all occurrences of entity_tokens in tokens (Continuous case)
        for i in range(len(tokens) - len(entity_tokens) + 1):
            if tokens[i:i + len(entity_tokens)] == entity_tokens:
                entity_indices.append((i, i + len(entity_tokens) - 1))

        # If contiguous match found, store it
        if entity_indices:
            offsets = ",".join(f"{start},{end}" for start, end in entity_indices)
            entity_mappings.append(f"{offsets} {entity_type}")
        else:
            # Handling discontinuous entities
            segment_indices = []
            remaining_tokens = entity_tokens[:]

            while remaining_tokens:
                found = False
                for size in range(len(remaining_tokens), 0, -1):  # Try decreasing segment sizes
                    segment = remaining_tokens[:size]
                    for i in range(len(tokens) - size + 1):
                        if tokens[i:i + size] == segment:
                            segment_indices.append((i, i + size - 1))
                            remaining_tokens = remaining_tokens[size:]  # Remove matched segment
                            found = True
                            break
                    if found:
                        break  # Restart with remaining tokens

                if not found:
                    remaining_tokens.pop(0)  # If no match found, drop the first token and retry

            # Format the discontinuous entity offsets
            if segment_indices:
                offsets = ",".join(f"{start},{end}" for start, end in segment_indices)
                entity_mappings.append(f"{offsets} {entity_type}")

    inline_spans = "|".join(entity_mappings)

    return formatted_text + "\n" + (inline_spans if inline_spans else "") + "\n\n"

df = pd.read_csv("/content/T5-Large.tsv", sep="\t")
inline_results = [convert_to_inline_format(row["Text"], row["Output"]) for _, row in df.iterrows()]
with open("output.txt", "w", encoding="utf-8") as f:
    f.writelines(inline_results)

print("Conversion complete. Check output.txt.")
