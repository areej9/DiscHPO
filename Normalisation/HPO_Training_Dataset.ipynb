{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhkojO19q81i"
      },
      "outputs": [],
      "source": [
        "# this code is to create a training set to train the sentence transformer model\n",
        "# that I will be using for Entity Linking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmm-jgpDRvdj",
        "outputId": "7dacd718-4a2a-4bba-8778-a588ae7d59ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jMf5TYBTrqs"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8G5z4XRTbVq"
      },
      "outputs": [],
      "source": [
        "# 1. I will resolve the span-offsets in training set to span strings:\n",
        "def convert_span_to_words(text, span):\n",
        "    doc = nlp(text)\n",
        "    span_parts = span.split(',')\n",
        "    spans = []\n",
        "    fullspan = \"\"\n",
        "    for span_part in span_parts:\n",
        "        span_range = span_part.split('-')\n",
        "\n",
        "        if len(span_range) >= 2:\n",
        "            start_char = span_range[0]\n",
        "            end_char = span_range[1]\n",
        "            start_token = None\n",
        "            end_token = None\n",
        "\n",
        "            for token in doc:\n",
        "                if token.idx == int(start_char):\n",
        "                    start_token = token\n",
        "                if token.idx == int(end_char):\n",
        "                    end_token = token\n",
        "                    break\n",
        "                elif token.idx > int(end_char):\n",
        "                    # Check if the end_char corresponds to whitespace\n",
        "                    if text[int(end_char)].isspace():\n",
        "                        end_token = doc[token.i - 1]\n",
        "                    break\n",
        "                elif token.i == len(doc) - 1:\n",
        "                    if int(end_char)>= len(doc):\n",
        "                       end_token = doc[token.i]\n",
        "                    break\n",
        "\n",
        "            if start_token and end_token:\n",
        "                start_word = start_token.text\n",
        "                end_word = end_token.text\n",
        "                span_words = doc[start_token.i: end_token.i + 1]\n",
        "                span_text = ' '.join([token.text for token in span_words])\n",
        "                fullspan = fullspan + span_text # in case I had disjoint spans\n",
        "                fullspan = fullspan + ' '\n",
        "                if span_part == span_parts[-1]:\n",
        "                  fullspan = fullspan.rstrip(' ')\n",
        "                  fullspan = fullspan.rstrip('/;.,')\n",
        "                  fullspan = fullspan.rstrip(' ')\n",
        "                  spans.append((fullspan))\n",
        "\n",
        "    return spans if spans else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQVmjjlaJ5-u",
        "outputId": "21952488-caa7-47b3-df86-d2a5299450f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7-21\n",
            "HP:0000219\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")  # Load the English language model\n",
        "texts = []\n",
        "span_offs = []\n",
        "hpoIDs_training = []\n",
        "Spans = []\n",
        "with open(\"/content/drive/MyDrive/biocreative/dataset/BioCreativeVIII3_TrainSet.tsv\", 'r', encoding='utf-8') as tsv_file:\n",
        "    reader = csv.reader(tsv_file, delimiter='\\t')\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        texts.append(row[1])\n",
        "        span_offs.append(row[4])\n",
        "        hpoIDs_training.append(row[2])\n",
        "print(span_offs[4]) # عشان تتأكدين\n",
        "print(hpoIDs_training[4])\n",
        "for text, span_off in zip(texts, span_offs):\n",
        "    span_list = convert_span_to_words(text, span_off)  # Get the list\n",
        "    if span_list:  # Check if the list is not empty\n",
        "        span = span_list[0]  # Get the first (and only) element from the list\n",
        "        Spans.append(span)  # Append the span to the Spans list\n",
        "    else:\n",
        "        Spans.append(\"NA\")  # Or some appropriate value if the list is empty\n",
        "\n",
        "# Spans column is ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYf-fwYscYN1",
        "outputId": "720de397-8705-418a-8163-22e28454ebec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prognathism\n"
          ]
        }
      ],
      "source": [
        "print(Spans[6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v5AnDRjT_ph",
        "outputId": "24209917-1a15-4f9a-ff0f-99244c260b9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2767\n",
            "2767\n",
            "2767\n",
            "2767\n"
          ]
        }
      ],
      "source": [
        "# 2. Now I'll get the official terms and IDs to do the matching\n",
        "terms = []\n",
        "hpoIDs = []\n",
        "training_terms = []\n",
        "with open(\"/content/drive/MyDrive/biocreative/EvaluationScript/HP2Terms.tsv\", 'r', encoding='utf-8') as tsv_file:\n",
        "    reader = csv.reader(tsv_file, delimiter='\\t')\n",
        "    for row in reader:\n",
        "        terms.append(row[1])\n",
        "        hpoIDs.append(row[0])\n",
        "    for hpoID_training in hpoIDs_training:\n",
        "      found_flag = False  # Flag to indicate whether hpoID_training was found\n",
        "      for hpoID in hpoIDs:\n",
        "          if hpoID_training == \"NA\":\n",
        "              training_terms.append(\"NA\")\n",
        "              found_flag = True  # Set the flag as found\n",
        "              break\n",
        "          if hpoID_training == hpoID:\n",
        "              training_terms.append(terms[hpoIDs.index(hpoID)])\n",
        "              found_flag = True  # Set the flag as found\n",
        "              break\n",
        "      if not found_flag:\n",
        "          training_terms.append(\"NotFound\")\n",
        "\n",
        "print(len(training_terms))\n",
        "print(len(texts))\n",
        "print(len(Spans))\n",
        "print(len(hpoIDs_training))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_E_Ktw3afhA",
        "outputId": "dbeb4e6c-146e-44d2-811e-a66859cf5fdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TSV file created successfully\n"
          ]
        }
      ],
      "source": [
        "with open('output.tsv', 'w', newline='', encoding='utf-8') as tsvfile:\n",
        "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
        "    writer.writerow([\"text\", \"Spans\", \"Terms\",\"HPO_ID\", \"Label\"])  # Write header row\n",
        "    for text, span, term, hpoIDs_training in zip(texts, Spans, training_terms, hpoIDs_training):\n",
        "      if span != \"NA\":\n",
        "        writer.writerow([text, span, term, hpoIDs_training, \"1\"])\n",
        "print(\"TSV file created successfully\")\n",
        "########## Code for creating training dataset finishes here #########"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlT2u48ZhgHP"
      },
      "outputs": [],
      "source": [
        "# Now I want to remove Unobservable terms from the official HP2Terms file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-k-Lzadgj2QK",
        "outputId": "a079d679-015e-4e5e-f701-184d9f524802"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modified rows saved in 'ObservedTerms.tsv'.\n"
          ]
        }
      ],
      "source": [
        "excluded_terms = []\n",
        "with open('/content/drive/MyDrive/biocreative/HPO/UnobservableHPOTerms.tsv', 'r') as second_file:\n",
        "    for line in second_file:\n",
        "        excluded_term = line.split('\\t')[0]  # Assuming \"ExcludedTerm\" is the only column\n",
        "        excluded_terms.append(excluded_term)\n",
        "\n",
        "output_lines = []\n",
        "with open('/content/drive/MyDrive/biocreative/HPO/HP2Terms.tsv', 'r') as first_file:\n",
        "    for line in first_file:\n",
        "        hpo_id = line.split('\\t')[0]  # Assuming HPO_ID is the first column\n",
        "        if hpo_id not in excluded_terms:\n",
        "            output_lines.append(line)\n",
        "\n",
        "with open('ObservableTerms.tsv', 'w') as new_file:\n",
        "    new_file.writelines(output_lines)\n",
        "\n",
        "print(\"Modified rows saved in 'ObservedTerms.tsv'.\")\n",
        "# Done, now I have list of only the observed terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPsRh7AGo4CK",
        "outputId": "09a792c1-3f6f-406a-b886-104b2a4868f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11715"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the total of ObservedTerms\n",
        "16908 - 5193"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu_twt8ep1EB"
      },
      "outputs": [],
      "source": [
        "# NOW, I want to append observable terms with the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waHS_q_QtBeX",
        "outputId": "11b1b867-0353-48f0-8b1f-306da29f8d0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appended data saved in 'AppendedTraining.tsv'.\n"
          ]
        }
      ],
      "source": [
        "spans = []\n",
        "terms = []\n",
        "preferred_terms = []\n",
        "\n",
        "with open('/content/drive/MyDrive/biocreative/Linking/Linking_Training.tsv', 'r') as training_file:\n",
        "    for line in training_file:\n",
        "        parts = line.strip().split('\\t')\n",
        "        if len(parts) >= 3:\n",
        "            span = parts[1]  # Assuming \"Spans\" is the second column\n",
        "            term = parts[2]  # Assuming \"Terms\" is the third column\n",
        "            spans.append(span)\n",
        "            terms.append(term)\n",
        "\n",
        "with open('/content/drive/MyDrive/biocreative/HPO/ObservableTerms.tsv', 'r') as observable_file:\n",
        "    for line in observable_file:\n",
        "        parts = line.strip().split('\\t')\n",
        "        if len(parts) >= 2:\n",
        "            preferred_term = parts[1]  # Assuming \"Preferred_Term\" is the second column\n",
        "            preferred_terms.append(preferred_term)\n",
        "\n",
        "spans += preferred_terms\n",
        "terms += preferred_terms\n",
        "\n",
        "with open('AppendedTraining.tsv', 'w') as output_file:\n",
        "    for span, term in zip(spans, terms):\n",
        "        output_file.write(f\"{span}\\t{term}\\n\")\n",
        "\n",
        "print(\"Appended data saved in 'AppendedTraining.tsv'.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
